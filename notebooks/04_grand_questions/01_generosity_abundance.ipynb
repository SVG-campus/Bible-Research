{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. The Generosity-Abundance Link (Direct-Execution Edition)\n",
    "## A Unified Causal Analysis\n",
    "\n",
    "**Objective:** Mathematically prove the relationship between Generosity (Giving) and Community Flourishing.\n",
    "\n",
    "**Note:** To avoid import errors, all Framework classes are defined directly in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ALL SYSTEMS ONLINE.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 1. DEFINE ALL FRAMEWORKS DIRECTLY (Bypassing File System Issues)\n",
    "# ------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "from scipy.stats import pearsonr, ks_2samp, norm, qmc\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.ensemble import RandomForestRegressor, IsolationForest\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(2025)\n",
    "np.random.seed(2025)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- FRAMEWORK 1: DISCOVERY ---\n",
    "class UnifiedDiscoveryEngine:\n",
    "    def __init__(self, df, target_col, date_col='Date_Index'):\n",
    "        self.raw_df = df.copy()\n",
    "        self.target = target_col\n",
    "        self.known_features = [c for c in df.columns if c not in [target_col, date_col]]\n",
    "        print(f\"üöÄ DISCOVERY ENGINE ONLINE. Target: '{self.target}'\")\n",
    "\n",
    "    def scan_environment(self):\n",
    "        print(\"\\n>> [PHASE 1] SCANNING ENVIRONMENT...\")\n",
    "        X = self.raw_df[self.known_features]\n",
    "        y = self.raw_df[self.target]\n",
    "        self.model = xgb.XGBRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "        self.model.fit(X, y)\n",
    "        print(f\"   Baseline Model R¬≤: {self.model.score(X, y):.4f}\")\n",
    "\n",
    "    def add_feature(self, name, data):\n",
    "        self.raw_df[name] = data\n",
    "        if name not in self.known_features: self.known_features.append(name)\n",
    "        print(f\"   ‚ûï ADDED: '{name}'\")\n",
    "\n",
    "# --- FRAMEWORK 2: ATTRIBUTION (FIXED) ---\n",
    "class UniversalAttributionValidator:\n",
    "    def __init__(self, X_raw=None, y_raw=None):\n",
    "        print(f\"üöÄ ATTRIBUTION ENGINE ONLINE on {DEVICE}...\")\n",
    "        self.model = None\n",
    "        if X_raw is not None and y_raw is not None:\n",
    "            self.fit(X_raw, y_raw)\n",
    "    \n",
    "    def fit(self, X_raw, y_raw, feature_names=None):\n",
    "        self.X_raw = X_raw\n",
    "        self.y_raw = y_raw\n",
    "        self.features = feature_names if feature_names else list(X_raw.columns)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_tensor = torch.FloatTensor(self.scaler.fit_transform(self.X_raw)).to(DEVICE)\n",
    "        y_values = self.y_raw.values\n",
    "        if len(y_values.shape) == 1: y_values = y_values.reshape(-1, 1)\n",
    "        self.y_tensor = torch.FloatTensor(y_values).to(DEVICE)\n",
    "        self.n_feat = self.X_tensor.shape[1]\n",
    "        self._train_robust_proxy()\n",
    "\n",
    "    def _train_robust_proxy(self):\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.n_feat, 48), nn.ReLU(), nn.Dropout(0.1), \n",
    "            nn.Linear(48, 24), nn.ReLU(), nn.Linear(24, 1)\n",
    "        ).to(DEVICE)\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.005)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        for _ in range(300):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = self.model(self.X_tensor)\n",
    "            loss = loss_fn(y_pred, self.y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"   Model Trained. Loss: {loss.item():.4f}\")\n",
    "\n",
    "    def compute_attribution(self, steps=50):\n",
    "        baseline = torch.zeros_like(self.X_tensor)\n",
    "        attributions = []\n",
    "        alphas = torch.linspace(0, 1, steps).to(DEVICE)\n",
    "        for i in range(len(self.X_tensor)):\n",
    "            path = baseline[i] + alphas.view(-1, 1) * (self.X_tensor[i] - baseline[i])\n",
    "            path.requires_grad = True\n",
    "            preds = self.model(path)\n",
    "            grads = torch.autograd.grad(torch.sum(preds), path)[0]\n",
    "            attr = (self.X_tensor[i] - baseline[i]) * torch.mean(grads, dim=0)\n",
    "            attributions.append(attr.detach().cpu().numpy())\n",
    "        self.ig_scores = pd.DataFrame(attributions, columns=self.features)\n",
    "        return self.ig_scores.mean().sort_values(ascending=False)\n",
    "\n",
    "# --- FRAMEWORK 3: INTERVENTION ---\n",
    "class PlatinumCausalEngine:\n",
    "    def __init__(self, causal_graph, data: pd.DataFrame):\n",
    "        self.G = nx.DiGraph(causal_graph)\n",
    "        self.df = data.copy()\n",
    "        self.nodes = list(nx.topological_sort(self.G))\n",
    "        self.models = {}\n",
    "        self.residuals = pd.DataFrame(index=self.df.index)\n",
    "        self._fit_adaptive_models()\n",
    "\n",
    "    def _fit_adaptive_models(self):\n",
    "        print(\"‚öôÔ∏è FITTING CAUSAL MODELS...\")\n",
    "        for node in self.nodes:\n",
    "            parents = list(self.G.predecessors(node))\n",
    "            if not parents:\n",
    "                self.residuals[node] = self.df[node]\n",
    "                continue\n",
    "            X = self.df[parents]\n",
    "            y = self.df[node]\n",
    "            self.models[node] = xgb.XGBRegressor(n_estimators=50, max_depth=3).fit(X, y)\n",
    "            self.residuals[node] = y - self.models[node].predict(X)\n",
    "\n",
    "    def simulate_intervention(self, treatment: dict, target: str):\n",
    "        df_sim = self.df.copy()\n",
    "        for t_var, t_val in treatment.items():\n",
    "            df_sim[t_var] = t_val\n",
    "        for node in self.nodes:\n",
    "            if node in treatment: continue\n",
    "            parents = list(self.G.predecessors(node))\n",
    "            if not parents: continue\n",
    "            X = df_sim[parents]\n",
    "            base_val = self.models[node].predict(X)\n",
    "            df_sim[node] = base_val + self.residuals[node].values\n",
    "        return df_sim[target].mean()\n",
    "\n",
    "# --- FRAMEWORK 4: OPTIMIZATION ---\n",
    "class UnifiedOptimizer:\n",
    "    def __init__(self, objective_fn, bounds):\n",
    "        self.objective_fn = objective_fn\n",
    "        self.bounds = bounds\n",
    "    \n",
    "    def optimize(self):\n",
    "        print(\"üîµ OPTIMIZING...\")\n",
    "        # Simple Grid Search for reliability in this demo\n",
    "        best_val = np.inf\n",
    "        best_param = 0\n",
    "        p_name = list(self.bounds.keys())[0]\n",
    "        low, high = self.bounds[p_name]\n",
    "        \n",
    "        for val in np.linspace(low, high, 20):\n",
    "            score = self.objective_fn(**{p_name: val})\n",
    "            if score < best_val:\n",
    "                best_val = score\n",
    "                best_param = val\n",
    "        return {p_name: best_param}, best_val\n",
    "\n",
    "# --- FRAMEWORK 5: VALIDATION ---\n",
    "class TitanValidationFramework:\n",
    "    def __init__(self, reference_data):\n",
    "        self.reference = reference_data\n",
    "        print(f\"üîç VALIDATION ENGINE ONLINE. Ref Size: {len(reference_data)}\")\n",
    "\n",
    "    def validate(self, new_data, target_col=None, subgroups=None):\n",
    "        report = []\n",
    "        # 1. Drift Check\n",
    "        for c in new_data.select_dtypes(include=np.number).columns:\n",
    "            if ks_2samp(self.reference[c], new_data[c])[1] < 0.05:\n",
    "                report.append(f\"Drift Detected: {c}\")\n",
    "        # 2. Fairness Check\n",
    "        if target_col and subgroups:\n",
    "            for g in subgroups:\n",
    "                means = new_data.groupby(g)[target_col].mean()\n",
    "                if (means.max() - means.min()) / means.min() > 0.2:\n",
    "                    report.append(f\"Fairness Alert: {g}\")\n",
    "        \n",
    "        return len(report) == 0, pd.DataFrame(report, columns=[\"Issues\"])\n",
    "\n",
    "print(\"‚úÖ ALL SYSTEMS ONLINE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç GENERATING DATASET (n=5000)...\n",
      "   Community_ID  Urban_Density  Median_Income  Giving_Percent  Crime_Rate  \\\n",
      "0             0       0.353677   38629.834970        0.083575    0.209689   \n",
      "1             1       0.248558   41026.762393        0.081281    0.161717   \n",
      "2             2       0.415959   48004.363098        0.092888    0.222203   \n",
      "3             3       0.159968   37374.099255        0.094659    0.090666   \n",
      "4             4       0.550283   61271.821009        0.072220    0.330701   \n",
      "\n",
      "   Flourishing_Index  \n",
      "0           0.147771  \n",
      "1           0.363633  \n",
      "2          -0.643565  \n",
      "3           1.634045  \n",
      "4          -1.002408  \n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# 2. GENERATE GRAND DATA (5,000 Rows)\n",
    "# --------------------------------------------------------\n",
    "def generate_grand_data(n=5000):\n",
    "    print(f\"üåç GENERATING DATASET (n={n})...\")\n",
    "    np.random.seed(42)\n",
    "    urban_density = np.random.beta(2, 5, n)\n",
    "    median_income = 30000 + (50000 * urban_density) + np.random.normal(0, 5000, n)\n",
    "    giving_percent = np.clip(0.12 - (0.1 * urban_density) + np.random.normal(0, 0.01, n), 0.01, 0.25)\n",
    "    crime_rate = np.clip(0.2 + (0.5 * urban_density) - (2.0 * giving_percent), 0, 1)\n",
    "    flourishing = (0.00001 * median_income) + (10 * giving_percent) - (5 * crime_rate) + np.random.normal(0, 0.5, n)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Community_ID': range(n), 'Urban_Density': urban_density, \n",
    "        'Median_Income': median_income, 'Giving_Percent': giving_percent, \n",
    "        'Crime_Rate': crime_rate, 'Flourishing_Index': flourishing\n",
    "    })\n",
    "\n",
    "df_grand = generate_grand_data()\n",
    "print(df_grand.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ DISCOVERY ENGINE ONLINE. Target: 'Flourishing_Index'\n",
      "\n",
      ">> [PHASE 1] SCANNING ENVIRONMENT...\n",
      "   Baseline Model R¬≤: 0.7063\n",
      "üöÄ ATTRIBUTION ENGINE ONLINE on cpu...\n",
      "   Model Trained. Loss: 0.2497\n",
      "\n",
      "Attribution Scores:\n",
      " Crime_Rate        0.028003\n",
      "Median_Income    -0.000329\n",
      "Giving_Percent   -0.019961\n",
      "Urban_Density    -0.029381\n",
      "dtype: float32\n",
      "‚öôÔ∏è FITTING CAUSAL MODELS...\n",
      "\n",
      "Scenario 2% Giving: -0.8162\n",
      "Scenario 10% Giving: 0.7039\n",
      "Lift: -186.24%\n",
      "üîµ OPTIMIZING...\n",
      "\n",
      "Optimal Giving Rate: 13.21%\n",
      "üîç VALIDATION ENGINE ONLINE. Ref Size: 5000\n",
      "\n",
      "Validation Passed: True\n",
      "Empty DataFrame\n",
      "Columns: [Issues]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# 3. EXECUTE FULL PIPELINE\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# Phase 1\n",
    "engine = UnifiedDiscoveryEngine(df_grand, 'Flourishing_Index')\n",
    "engine.scan_environment()\n",
    "\n",
    "# Phase 2\n",
    "uav = UniversalAttributionValidator()\n",
    "uav.fit(df_grand[['Giving_Percent', 'Median_Income', 'Crime_Rate', 'Urban_Density']], df_grand['Flourishing_Index'])\n",
    "print(\"\\nAttribution Scores:\\n\", uav.compute_attribution())\n",
    "\n",
    "# Phase 3\n",
    "graph = [('Urban_Density', 'Crime_Rate'), ('Giving_Percent', 'Crime_Rate'), \n",
    "         ('Crime_Rate', 'Flourishing_Index'), ('Giving_Percent', 'Flourishing_Index')]\n",
    "pce = PlatinumCausalEngine(graph, df_grand)\n",
    "res_a = pce.simulate_intervention({'Giving_Percent': 0.02}, 'Flourishing_Index')\n",
    "res_b = pce.simulate_intervention({'Giving_Percent': 0.10}, 'Flourishing_Index')\n",
    "print(f\"\\nScenario 2% Giving: {res_a:.4f}\")\n",
    "print(f\"Scenario 10% Giving: {res_b:.4f}\")\n",
    "print(f\"Lift: {((res_b - res_a)/res_a):.2%}\")\n",
    "\n",
    "# Phase 4\n",
    "def obj(giving_rate): return -pce.simulate_intervention({'Giving_Percent': giving_rate}, 'Flourishing_Index')\n",
    "opt = UnifiedOptimizer(obj, {'giving_rate': (0.01, 0.30)})\n",
    "best, val = opt.optimize()\n",
    "print(f\"\\nOptimal Giving Rate: {best['giving_rate']:.2%}\")\n",
    "\n",
    "# Phase 5\n",
    "fsv = TitanValidationFramework(df_grand)\n",
    "v, r = fsv.validate(df_grand.sample(500), 'Flourishing_Index', ['Urban_Density'])\n",
    "print(\"\\nValidation Passed:\", v)\n",
    "print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
